# Relat√≥rio Final - Projeto de Disciplina de Processamento de Linguagem Natural com Python

Este relat√≥rio detalha as respostas √†s quest√µes propostas no trabalho final da disciplina, complementando o notebook `projeto_nlp.ipynb` onde o c√≥digo foi implementado e as an√°lises foram realizadas.

## Implementar t√©cnicas de lematiza√ß√£o

**Qual o endere√ßo do seu notebook (colab) executado? Use o bot√£o de compartilhamento do colab para obter uma url.**

[clique aqui para ver o notebook do projeto](https://colab.research.google.com/drive/1geunqeoRkUpoog7VM5F92qu_qtxY1cdK#scrollTo=s_NpsAsHItOr)

**Em qual c√©lula est√° o c√≥digo que realiza o download dos pacotes necess√°rios para tokeniza√ß√£o e stemming usando nltk?**

O c√≥digo que realiza o download dos pacotes `stopwords`, `punkt` e `rslp` do NLTK est√° na c√©lula com o ID. O c√≥digo adicionado foi:

```python
import nltk
nltk.download("stopwords")
nltk.download("punkt")
nltk.download("punkt_tab")
nltk.download("rslp")
```

**Em qual c√©lula est√° o c√≥digo que atualiza o spacy e instala o pacote pt_core_news_lg?**

C√≥digo que atualiza a biblioteca Spacy e instala o modelo `pt_core_news_lg` para portugu√™s:

```python
!pip install -U spacy
!python -m spacy download pt_core_news_lg

import spacy
from spacy.lang.pt.stop_words import STOP_WORDS
```

**Em qual c√©lula est√° o download dos dados diretamente do kaggle?**

C√≥digo respons√°vel por baixar o dataset diretamente do Kaggle usando a API:

```bash
!kaggle datasets download --force -d marlesson/news-of-the-site-folhauol
```

**Em qual c√©lula est√° a cria√ß√£o do dataframe news_2016 (com examente 7943 not√≠cias)?**

C√≥digo de cria√ß√£o do dataframe `news_2016`, filtrando as not√≠cias do ano de 2016 e da categoria "mercado":

```python
df['date'] = pd.to_datetime(df.date)
news_2016 = df[(df["date"].dt.year == 2016) & (df["category"].str.lower() == "mercado")].copy()
```

**Em qual c√©lula est√° a fun√ß√£o que tokeniza e realiza o stemming dos textos usando fun√ß√µes do nltk?**

Fun√ß√£o `tokenize` que realiza a tokeniza√ß√£o e o stemming (usando `RSLPStemmer`) com NLTK:

```python
from nltk.tokenize import word_tokenize
from nltk.stem import RSLPStemmer

def tokenize(text: str) -> List:
  stemmer = RSLPStemmer()
  tokens = word_tokenize(text.lower())
  stems = [stemmer.stem(token) for token in tokens if token.isalpha()]
  return stems
  
news_2016.loc[:, 'nltk_tokens'] = news_2016.text.progress_map(tokenize)
```

**Em qual c√©lula est√° a fun√ß√£o que realiza a lematiza√ß√£o usando o spacy?**

A lematiza√ß√£o usando Spacy √© realizada atrav√©s de duas fun√ß√µes auxiliares (`filter` e `lemma`). A fun√ß√£o `lemma` aplica a lematiza√ß√£o e utiliza a fun√ß√£o `filter` para remover stopwords e tokens indesejados:

Na fun√ß√£o `filter`:
```python
def filter(w: spacy.lang.pt.Portuguese) -> bool:
  return w.is_alpha and w.text.lower() not in complete_stopwords and w.lemma_ not in ['o', 'em', 'em o', 'em a', 'ano']
```

Na fun√ß√£o `lemma`:
```python
def lemma(doc: spacy.lang.pt.Portuguese) -> List[str]:
  lemmas = [w.lemma_ for w in doc if filter(w)]
  return lemmas
```

**Baseado nos resultados qual a diferen√ßa entre stemming e lematiza√ß√£o, qual a diferen√ßa entre os dois procedimentos? Escolha quatro palavras para exemplificar.**

A principal diferen√ßa entre stemming (radicaliza√ß√£o) e lematiza√ß√£o reside na abordagem para reduzir as palavras √† sua forma base:

*   **Stemming:** √â um processo mais heur√≠stico e r√°pido que remove sufixos (e √†s vezes prefixos) das palavras para obter um radical comum. O resultado nem sempre √© uma palavra v√°lida do dicion√°rio. O objetivo √© agrupar palavras com o mesmo significado conceitual, mesmo que a forma resultante n√£o seja linguisticamente correta.
*   **Lematiza√ß√£o:** √â um processo mais sofisticado e geralmente mais lento que utiliza an√°lise de um dicion√°rio para encontrar o lema de uma palavra. O resultado da lematiza√ß√£o √© sempre uma palavra v√°lida do dicion√°rio. Leva em considera√ß√£o o contexto da palavra (classe gramatical, por exemplo) para determinar o lema correto.

A diferen√ßa √© que a lematiza√ß√£o produz palavras reais, enquanto o stemming pode produzir palavras n√£o reais, focando apenas em truncar a palavra.

**Exemplos (considerando o contexto do portugu√™s e as bibliotecas usadas):**

| Palavra Original | Stemming (RSLPStemmer - NLTK) | Lematiza√ß√£o (pt_core_news_lg - Spacy) |
| :--------------- | :---------------------------- | :------------------------------------ |
| `correndo`       | `corr`                        | `correr`                              |
| `casas`          | `cas`                         | `casa`                                |
| `falavam`        | `fal`                         | `falar`                               |
| `felizmente`     | `feliz`                       | `felizmente` (ou `feliz` dependendo do contexto/modelo) |

Nestes exemplos, vemos que o stemming (RSLP) reduz as palavras a radicais mais curtos (`corr`, `cas`, `fal`), que n√£o s√£o necessariamente palavras completas. A lematiza√ß√£o, por outro lado, retorna o infinitivo do verbo (`correr`, `falar`) ou a forma singular do substantivo (`casa`), que s√£o formas v√°lidas no dicion√°rio.




## Construir um modelo de reconhecimento de entidades (NER) usando Spacy

**Em qual c√©lula o modelo pt_core_news_lg est√° sendo carregado? Todos os textos do dataframe precisam ser analisados usando os modelos carregados. Em qual c√©lula isso foi feito?**

O modelo `pt_core_news_lg` do Spacy √© carregado na c√©lula com ID. Nesta mesma c√©lula, o modelo √© aplicado a todos os textos da coluna `text` do dataframe `news_2016` para criar os documentos Spacy, que s√£o armazenados na nova coluna `spacy_doc`. O c√≥digo utilizado foi:

C√≥digo de carregamento do modelo `pt_core_news_lg` nesse mesmo trecho o modelo √© aplicado a todos os textos da coluna `text` do dataframe `news_2016` para criar os documentos Spacy, que s√£o armazenados na nova coluna `spacy_doc`.

```python
  nlp = spacy.load("pt_core_news_lg")
  news_2016['spacy_doc'] = list(nlp.pipe(news_2016['text']))
```

**Indique a c√©lula onde as entidades dos textos foram extra√≠das. Estamos interessados apenas nas organiza√ß√µes.**

A extra√ß√£o das entidades do tipo "Organiza√ß√£o" (ORG) √© realizada pela fun√ß√£o `NER`. Esta fun√ß√£o processa cada documento Spacy (da coluna `spacy_doc`) e extrai apenas as entidades rotuladas como `ORG`. O resultado √© armazenado na coluna `spacy_ner`. O c√≥digo preenchido na fun√ß√£o foi:

```python
def NER(doc: spacy.lang.pt.Portuguese):
  organizations = [ent.text for ent in doc.ents if ent.label_ == 'ORG']
  return organizations

news_2016.loc[:, 'spacy_ner'] = news_2016.spacy_doc.progress_map(NER)
```

**Cole a figura gerada que mostra a nuvem de entidades para cada t√≥pico obtido (no final do notebook)**

A figura com a nuvem de entidades para cada um dos 9 t√≥picos

![Nuvem de entidades por t√≥picos](imagens/nuvens_entidades.png)

## Criar modelos utilizando vetoriza√ß√£o de textos baseado em Bag of Words

**Quando adotamos uma estrat√©gia frequentista para converter textos em vetores, podemos faz√™-lo de diferentes maneiras. Mostramos em aula as codifica√ß√µes One-Hot, TF e TF-IDF. Explique a principal motiva√ß√£o em adotar TF-IDF frente as duas outras op√ß√µes.**

Foi utilizado TF-IDF em vez de One-Hot ou TF pela sua capacidade de **ponderar a import√¢ncia das palavras** n√£o apenas pela sua frequ√™ncia dentro de um documento, mas tamb√©m pela sua raridade em toda a cole√ß√£o de documentos (corpus).

*   **One-Hot:** Representa cada palavra como um vetor esparso onde apenas a posi√ß√£o correspondente √† palavra √© 1 e as outras s√£o 0. Ignora a frequ√™ncia da palavra no documento e a dimensionalidade cresce linearmente com o tamanho do vocabul√°rio, tornando-se invi√°vel para grandes corpus.
*   **TF:** Conta a frequ√™ncia de cada palavra em um documento. D√° mais peso a palavras que aparecem muitas vezes, mas pode supervalorizar palavras comuns (como artigos e preposi√ß√µes, mesmo ap√≥s a remo√ß√£o de stopwords) que aparecem frequentemente em muitos documentos, mas carregam pouca informa√ß√£o distintiva sobre o conte√∫do espec√≠fico do documento.
*   **TF-IDF:** Combina a frequ√™ncia do termo (TF) com a frequ√™ncia inversa do documento (IDF). O IDF mede qu√£o comum ou rara uma palavra √© em todo o corpus. Palavras que aparecem em muitos documentos ter√£o um IDF baixo, enquanto palavras raras ter√£o um IDF alto. Ao multiplicar TF por IDF, o TF-IDF atribui um peso maior a palavras que s√£o frequentes em um documento espec√≠fico, mas raras no corpus geral. Isso ajuda a destacar termos que s√£o realmente importantes e distintivos para o conte√∫do daquele documento, ao mesmo tempo que diminui o peso de palavras muito comuns e pouco informativas.

Portanto, o TF-IDF oferece uma representa√ß√£o vetorial mais significativa e discriminativa do conte√∫do textual em compara√ß√£o com One-Hot e TF puro, sendo √∫til para tarefas como classifica√ß√£o de texto, clustering e recupera√ß√£o de informa√ß√£o.

**Indique a c√©lula onde est√° a fun√ß√£o que cria o vetor de TF-IDF para cada texto.**

A cria√ß√£o do vetor TF-IDF √© realizada dentro da classe `Vectorizer`, no m√©todo `vectorizer`. O vetorizador `TfidfVectorizer` do Scikit-learn √© inicializado e treinado (fit) com os tokens lematizados. A aplica√ß√£o para gerar a coluna `tfidf` no dataframe usa a fun√ß√£o `tokens2tfidf`. foi exclu√≠do do docs tbm todas as palavras com menos de 3 digitos. O c√≥digo preenchido no m√©todo `vectorizer` foi:

```python
  def vectorizer(self):
      docs = [" ".join([t for t in tokens if len(t) >= 3]) for tokens in self.doc_tokens]

      self.tfidf = TfidfVectorizer(
          lowercase=True,
          stop_words=list(complete_stopwords),
          max_features=5000,
          min_df=10,
          tokenizer=lambda x: x.split(),
          preprocessor=lambda x: x,
          ngram_range=(1, 2)  
      ).fit(docs)

      return self.tfidf
```

**Indique a c√©lula onde est√£o sendo extra√≠dos os t√≥picos usando o algoritmo de LDA.**

C√≥digo de extra√ß√£o dos t√≥picos usando o algoritmo LDA do Scikit-learn. O modelo LDA √© inicializado com 9 componentes (t√≥picos), 100 itera√ß√µes m√°ximas e a semente aleat√≥ria definida, e ent√£o treinado (fit) com a matriz TF-IDF (corpus). O c√≥digo preenchido foi:

```python
N_TOKENS = 9

corpus = np.stack(news_2016['tfidf'].values)
lda = LDA(n_components=N_TOKENS, max_iter=100, random_state=SEED)
lda.fit(corpus)
```

**Indique a c√©lula onde a visualiza√ß√£o LDAVis est√° criada.**


**Cole a figura com a nuvem de palavras para cada um dos 9 t√≥picos criados.**

A figura com a nuvem de palavras para cada um dos 9 t√≥picos.

![](imagens/nuvens_palavras.png)

**Escreva brevemente uma descri√ß√£o para cada t√≥pico extra√≠do. Indique se voc√™ considera o t√≥pico extra√≠do semanticamente consistente ou n√£o.**

*(Esta se√ß√£o ser√° preenchida ap√≥s a execu√ß√£o do notebook e an√°lise das nuvens de palavras e/ou dos termos mais prov√°veis de cada t√≥pico gerado pelo LDA. A consist√™ncia sem√¢ntica ser√° avaliada com base na coes√£o e interpretabilidade das palavras associadas a cada t√≥pico.)*

## Criar modelos baseados em Word Embedding

**Neste projeto, usamos TF-IDF para gerar os vetores que servem de entrada para o algoritmo de LDA. Quais seriam os passos para gerar vetores baseados na t√©cnica de Doc2Vec?**

Para gerar vetores de documentos usando Doc2Vec, os passos seriam os seguintes:

1.  **Prepara√ß√£o dos Dados:** Assim como no TF-IDF, precisar√≠amos dos textos pr√©-processados (tokenizados, possivelmente lematizados ou stemizados, e com remo√ß√£o de stopwords). No entanto, o Doc2Vec requer um formato espec√≠fico: uma lista onde cada elemento √© um objeto `TaggedDocument` da biblioteca Gensim. Cada `TaggedDocument` cont√©m a lista de tokens do documento e uma tag √∫nica (que pode ser um √≠ndice num√©rico ou uma string identificadora do documento).
2.  **Instancia√ß√£o do Modelo:** Criar uma inst√¢ncia do modelo `Doc2Vec` da biblioteca Gensim. Seria necess√°rio definir par√¢metros como `vector_size` (a dimensionalidade dos vetores resultantes), `window` (o tamanho da janela de contexto), `min_count` (ignorar palavras com frequ√™ncia total menor que este valor), `workers` (n√∫mero de threads para treinamento), `epochs` (n√∫mero de itera√ß√µes sobre o corpus), e o algoritmo a ser usado (`dm=1` para PV-DM ou `dm=0` para PV-DBOW).
3.  **Constru√ß√£o do Vocabul√°rio:** Chamar o m√©todo `build_vocab()` no modelo Doc2Vec, passando a lista de `TaggedDocument`s. Isso constr√≥i o vocabul√°rio interno do modelo.
4.  **Treinamento do Modelo:** Chamar o m√©todo `train()` no modelo Doc2Vec, passando novamente a lista de `TaggedDocument`s, o n√∫mero total de exemplos (`total_examples=model.corpus_count`) e o n√∫mero de √©pocas (`epochs=model.epochs`). Este passo treina os vetores das palavras e dos documentos.
5.  **Extra√ß√£o dos Vetores:** Ap√≥s o treinamento, os vetores dos documentos est√£o armazenados no atributo `model.dv` (ou `model.docvecs` em vers√µes mais antigas do Gensim). Pode-se acessar o vetor de um documento espec√≠fico pela sua tag. Para obter uma matriz com os vetores de todos os documentos na ordem original, seria necess√°rio iterar pelas tags e recuperar os vetores correspondentes.


## Autor
Desenvolvido por **Herbert Fenando Jarenco de Souza Martins**  
üîó [GitHub Repository](https://github.com/herbertins/nlp-project)